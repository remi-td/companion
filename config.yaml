# Configuration file for the Companion AI assistant

llm-chat:
  path: "models/DeepSeek-R1-Distill-Llama-8B-Q8_0.gguf"  # Default model path
  temperature: 0.7  # Controls randomness (0 = deterministic, 1 = highly random)
  max_tokens: 0  # Maximum number of tokens in generated responses (0 = from model)
  top_p: 0.9  # Nucleus sampling for diversity
  n_ctx: 8000  # Context window size (0 = from model)
  n_gpu_layers: 20  # Number of layers to run on GPU (for acceleration)
  thinking_tag: ["<think>", "</think>"]  # Set to None to disable thinking extraction

llm-embed:
  path: "models/nomic-embed-text.gguf"
  n_gpu_layers: 0  # Default to CPU
  dim: 384  # Embedding vector size  

database:
  path: "data/companion.db"
  enable_rag: true  # Option to toggle RAG retrieval  