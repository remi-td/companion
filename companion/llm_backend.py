# llm_backend.py
"""
LLMBackend class that serves as the bridge between the front-end (UI) and the LLM model.
Handles prompt generation, passing context to the model, and retrieving relevant information
for enhanced responses using the RAG (Retrieval-Augmented Generation) pattern.
"""

from llm import ChatLLM
from database import Database
import re

class LLMBackend:
    def __init__(self):
        """
        Initialize the LLMBackend with a default or user-specified model path.
        """
        self.db = Database()
        self.current_thread_id = None
        self.llm_model = None  # Main LLM instance for chat and summarization
        self.summary_generated = False  # Flag to track if summary has been generated

    def create_new_thread(self, title='New Chat'):
        """
        Create a new chat thread and initialize the LLM model.

        Args:
            title (str): Title of the new chat thread.

        Returns:
            int: The ID of the newly created chat thread.
        """
        thread_id = self.db.create_thread(title)
        self.current_thread_id = thread_id
        self.summary_generated = False  # Reset the summary flag for new thread
        return thread_id

    def load_thread(self, thread_id):
        """
        Load an existing chat thread and initialize the LLM model with its conversation history.

        Args:
            thread_id (int): The ID of the chat thread to load.
        """
        self.current_thread_id = thread_id
        self.llm_model = ChatLLM()
        messages = self.db.get_messages(thread_id)
        self.llm_model.conversation_history = [
            {'role': role, 'content': content} for role, content in messages
        ]
        # Determine if summary has already been generated
        thread_title = self.db.get_thread_title(thread_id)
        self.summary_generated = thread_title != 'New Chat'  # Assuming 'New Chat' is default title

    def get_all_threads(self):
        """
        Retrieve all chat threads from the database.

        Returns:
            list of tuples: A list containing tuples of thread IDs and their titles.
        """
        return self.db.get_threads()

    def get_response_stream(self, user_message):
        """
        Process the user's message, generate a response stream, and update the conversation history.

        Args:
            user_message (str): The message input by the user.

        Returns:
            generator: A generator yielding chunks of the assistant's response.
        """
        if self.llm_model is None:
            self.load_thread(self.current_thread_id)

        # Process user message
        self.llm_model._update_conversation_history(user_message, role='user')
        self.db.save_message(self.current_thread_id, 'user', user_message)
        # Generate response stream
        response_stream = self.llm_model.generate_response(user_message, stream=True)
        return response_stream

    def save_assistant_response(self, assistant_response):
        """
        Save the assistant's response to the conversation history and database.
        Also triggers summarization after the first assistant response.

        Args:
            assistant_response (str): The response generated by the assistant.
        """

        # Extract the thinking trace if this a thinking model and tags defined in the config
        assistant_thinking=None
        if len(self.llm_model.thinking_tag)==2:
            pattern = re.escape(self.llm_model.thinking_tag[0]) + r"(.*?)" + re.escape(self.llm_model.thinking_tag[1])
            match = re.search(pattern, assistant_response, re.DOTALL)

            if match:
                assistant_thinking = match.group(1).strip()
                assistant_response = re.sub(pattern, "", assistant_response, flags=re.DOTALL).strip()

        self.llm_model._update_conversation_history(assistant_response, role='assistant')
        self.db.save_message(self.current_thread_id, 'assistant', assistant_response, traces=assistant_thinking)
        # Perform summarization after the first assistant response
        if not self.summary_generated:
            self.generate_summary()
            self.summary_generated = True

    def generate_summary(self):
        """
        Generate a summary of the conversation using the main LLM instance.
        The summary is based on the first user message and assistant response.
        """
        # Prepare the conversation text
        conversation_text = ""
        # Use only the first user message and assistant response
        if len(self.llm_model.conversation_history) >= 2:
            for message in self.llm_model.conversation_history[:2]:
                role = message['role']
                content = message['content']
                conversation_text += f"{role.capitalize()}: {content}\n"
        else:
            # Not enough messages to generate summary
            return

        # Define the prompt for summarization
        summary_prompt = (
            f"Provide a brief (1-2 words) summary of the main theme of the following conversation:\n\n"
            f"{conversation_text}\n\nSummary:"
        )

        try:
            # Generate the summary using the same LLM instance
            summary_data = self.llm_model.model.create_completion(
                prompt=summary_prompt,
                max_tokens=10,
                temperature=0.5,
                top_p=0.9,
            )
            summary = summary_data['choices'][0]['text'].strip()
            # Update the chat thread title in the database
            self.db.update_thread_title(self.current_thread_id, summary)
        except Exception as e:
            print(f"Error generating summary: {e}")